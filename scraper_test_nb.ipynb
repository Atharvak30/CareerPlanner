{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_linkedin_search_url(job_title, location, start=0):\n",
    "    \"\"\"\n",
    "    Constructs a properly formatted LinkedIn job search URL.\n",
    "\n",
    "    Args:\n",
    "        job_title: Job title to search for (e.g., \"Software Engineer\")\n",
    "        location: Location to search in (e.g., \"San Francisco, CA\" or \"remote\")\n",
    "        start: Pagination offset (0, 25, 50, etc.)\n",
    "\n",
    "    Returns:\n",
    "        Properly formatted and URL-encoded LinkedIn job search URL\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search\"\n",
    "\n",
    "    params = {\n",
    "        \"keywords\": job_title,\n",
    "        \"location\": location,\n",
    "        \"start\": start\n",
    "    }\n",
    "\n",
    "    query_string = urllib.parse.urlencode(params)\n",
    "    return f\"{base_url}?{query_string}\"\n",
    "\n",
    "\n",
    "def test_linkedin_connection(url):\n",
    "    \"\"\"\n",
    "    Tests HTTP connectivity to LinkedIn job search URL.\n",
    "\n",
    "    Args:\n",
    "        url: The URL to test\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (status_code, response_length)\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Referer\": \"https://www.linkedin.com/\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        return response.status_code, len(response.text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error connecting to LinkedIn: {e}\")\n",
    "        return None, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = build_linkedin_search_url(\"Data Scientist\", \"remote\")\n",
    "print(f\"Generated URL: {url}\\n\")\n",
    "\n",
    "status, length = test_linkedin_connection(url)\n",
    "\n",
    "if status:\n",
    "    print(f\"Status Code: {status}\")\n",
    "    print(f\"Response Length: {length} characters\")\n",
    "\n",
    "    # Verify expectations\n",
    "    if status == 200 and length > 1000:\n",
    "        print(\"\\n✓ Connection test passed!\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Connection test failed - Status: {status}, Length: {length}\")\n",
    "else:\n",
    "    print(\"✗ Connection failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_job_cards(html_content):\n",
    "    \"\"\"\n",
    "    Parses LinkedIn job search HTML to extract job listings.\n",
    "    \n",
    "    Args:\n",
    "        html_content: HTML string from LinkedIn job search\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing job information\n",
    "    \"\"\"\n",
    "    jobs = []\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Find all job card list items\n",
    "        job_cards = soup.find_all('li')\n",
    "        \n",
    "        for card in job_cards:\n",
    "            try:\n",
    "                # Extract job title\n",
    "                title_elem = card.find(class_='base-search-card__title')\n",
    "                job_title = title_elem.get_text(strip=True) if title_elem else \"N/A\"\n",
    "                \n",
    "                # Extract company name\n",
    "                company_elem = card.find(class_='base-search-card__subtitle')\n",
    "                company = company_elem.get_text(strip=True) if company_elem else \"N/A\"\n",
    "                \n",
    "                # Extract location\n",
    "                location_elem = card.find(class_='job-search-card__location')\n",
    "                location = location_elem.get_text(strip=True) if location_elem else \"N/A\"\n",
    "                \n",
    "                # Extract posted date\n",
    "                posted_elem = card.find(class_='job-search-card__listdate')\n",
    "                posted_ago = posted_elem.get_text(strip=True) if posted_elem else \"N/A\"\n",
    "                \n",
    "                # Extract job URL\n",
    "                link_elem = card.find('a', class_='base-card__full-link')\n",
    "                job_url = link_elem.get('href', 'N/A') if link_elem else \"N/A\"\n",
    "                \n",
    "                # Extract job ID from URL\n",
    "                job_id = None\n",
    "                if job_url != \"N/A\":\n",
    "                    match = re.search(r'/jobs/view/(\\d+)', job_url)\n",
    "                    if match:\n",
    "                        job_id = match.group(1)\n",
    "                \n",
    "                # Only add if we have at least a title and URL\n",
    "                if job_title != \"N/A\" and job_url != \"N/A\":\n",
    "                    jobs.append({\n",
    "                        'job_id': job_id,\n",
    "                        'job_title': job_title,\n",
    "                        'company': company,\n",
    "                        'location': location,\n",
    "                        'posted_ago': posted_ago,\n",
    "                        'job_url': job_url\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Skip individual cards that fail to parse\n",
    "                print(f\"Warning: Failed to parse a job card: {e}\")\n",
    "                continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing HTML content: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_jobs(title: str):\n",
    "    # Test with real request: Fetch Software Engineer jobs in remote\n",
    "    test_url = build_linkedin_search_url(title, \"remote\")\n",
    "    print(f\"Fetching jobs from: {test_url}\\n\")\n",
    "\n",
    "    # Make the request\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Referer\": \"https://www.linkedin.com/\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(test_url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"✓ Successfully fetched data (Status: {response.status_code})\\n\")\n",
    "            \n",
    "            # Parse the jobs\n",
    "            jobs = parse_job_cards(response.text)\n",
    "            \n",
    "            print(f\"Found {len(jobs)} jobs\\n\")\n",
    "            print(\"=\" * 80)\n",
    "            print(\"First 3 jobs:\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            for i, job in enumerate(jobs, 1):\n",
    "                print(f\"\\n{i}. {job['job_title']}\")\n",
    "                print(f\"   Company: {job['company']}\")\n",
    "                print(f\"   Location: {job['location']}\")\n",
    "                print(f\"   Posted: {job['posted_ago']}\")\n",
    "                print(f\"   Job ID: {job['job_id']}\")\n",
    "                print(f\"   URL: {job['job_url']}...\" if len(job['job_url']) > 80 else f\"   URL: {job['job_url']}\")\n",
    "                \n",
    "                try:\n",
    "                    # Verify job_url and job_id exist\n",
    "                    assert job['job_url'] is not None and job['job_url'] != \"N/A\", f\"Job {i} missing URL\"\n",
    "                    assert job['job_id'] is not None, f\"Job {i} missing ID\"\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"✓ All tests passed! All jobs have valid URLs and IDs.\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"✗ Failed to fetch data (Status: {response.status_code})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error during test: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_jobs(\"Machine Learning Engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobStorage:\n",
    "    \"\"\"\n",
    "    In-memory storage for job data with description management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.jobs = []\n",
    "    \n",
    "    def add_jobs(self, job_list):\n",
    "        \"\"\"\n",
    "        Store jobs in internal list.\n",
    "        \n",
    "        Args:\n",
    "            job_list: List of job dictionaries\n",
    "        \"\"\"\n",
    "        for job in job_list:\n",
    "            # Ensure each job has a description field\n",
    "            if 'description' not in job:\n",
    "                job['description'] = None\n",
    "            self.jobs.append(job)\n",
    "    \n",
    "    def get_all_jobs(self):\n",
    "        \"\"\"\n",
    "        Return all stored jobs.\n",
    "        \n",
    "        Returns:\n",
    "            List of all job dictionaries\n",
    "        \"\"\"\n",
    "        return self.jobs\n",
    "    \n",
    "    def get_jobs_without_description(self):\n",
    "        \"\"\"\n",
    "        Return jobs where description is None.\n",
    "        \n",
    "        Returns:\n",
    "            List of jobs without descriptions\n",
    "        \"\"\"\n",
    "        return [job for job in self.jobs if job.get('description') is None]\n",
    "    \n",
    "    def update_job_description(self, job_id, description):\n",
    "        \"\"\"\n",
    "        Update specific job's description.\n",
    "        \n",
    "        Args:\n",
    "            job_id: Job ID to update\n",
    "            description: Description text to set\n",
    "        \"\"\"\n",
    "        for job in self.jobs:\n",
    "            if job.get('job_id') == job_id:\n",
    "                job['description'] = description\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"\n",
    "        Return statistics about stored jobs.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with job statistics\n",
    "        \"\"\"\n",
    "        total = len(self.jobs)\n",
    "        with_desc = sum(1 for job in self.jobs if job.get('description') and job.get('description') != \"ERROR\")\n",
    "        without_desc = sum(1 for job in self.jobs if job.get('description') is None)\n",
    "        errors = sum(1 for job in self.jobs if job.get('description') == \"ERROR\")\n",
    "        \n",
    "        return {\n",
    "            'total_jobs': total,\n",
    "            'jobs_with_descriptions': with_desc,\n",
    "            'jobs_without_descriptions': without_desc,\n",
    "            'jobs_with_errors': errors\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Referer\": \"https://www.linkedin.com/jobs/\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, headers=headers, timeout=10)\n",
    "    print(f\"Status: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        desc_elem = (\n",
    "            soup.find('div', class_='show-more-less-html__markup') or\n",
    "            soup.find('div', class_='description__text') or\n",
    "            soup.find('section', class_='description') or\n",
    "            soup.find('div', class_='description') or\n",
    "            soup.find('article', class_='jobs-description__container') or\n",
    "            soup.find('div', class_=lambda x: x and 'jobs-description__content' in x) or\n",
    "            soup.find('div', class_=re.compile(r'job.?description', re.I))\n",
    "        )\n",
    "        \n",
    "        if desc_elem:\n",
    "            print(desc_elem, desc_elem.get_text(strip=True))\n",
    "            # return desc_elem.get_text(strip=True)\n",
    "    \n",
    "    # return None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    # return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_job_description(job_url):\n",
    "    \"\"\"\n",
    "    Fetch job description from LinkedIn job URL.\n",
    "    \n",
    "    Args:\n",
    "        job_url: URL to the job posting\n",
    "    \n",
    "    Returns:\n",
    "        Job description text or None if fetch fails\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Referer\": \"https://www.linkedin.com/\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(job_url, headers=headers, timeout=10)\n",
    "        \n",
    "        print(response)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            print(\"nigga 2\")\n",
    "            # Try multiple possible selectors for job description\n",
    "            desc_elem = (\n",
    "                soup.find('div', class_='show-more-less-html__markup') or\n",
    "                soup.find('div', class_='description__text') or\n",
    "                soup.find('section', class_='description') or\n",
    "                soup.find('div', class_='description') or\n",
    "                soup.find('article',class_='jobs-description__container jobs-description__container--condensed') or \n",
    "                soup.find('div',class_='jobs-description__content jobs-description-content jobs-description__content--condensed') or\n",
    "                soup.find('div', class_=re.compile(r'^job_description'))\n",
    "\n",
    "            )\n",
    "\n",
    "            print(\"nigga\")\n",
    "            \n",
    "            if desc_elem:\n",
    "                description = desc_elem.get_text(strip=True)\n",
    "                return description if description else None\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching description: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.linkedin.com/jobs/view/data-scientist-data-analytics-%E2%80%93-customer-loyalty-marketing-at-circle-k-4313197449\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = fetch_job_description('https://www.linkedin.com/jobs/view/data-scientist-data-analytics-%E2%80%93-customer-loyalty-marketing-at-circle-k-4316137579/?position=3&pageNum=0&refId=r8vm%2BrG%2B0ZTbNredBK8TzQ%3D%3D&trackingId=brKmbAor271tMuHOwExs2Q%3D%3D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_descriptions(storage, delay=3):\n",
    "    \"\"\"\n",
    "    Fetch descriptions for all jobs without descriptions.\n",
    "    \n",
    "    Args:\n",
    "        storage: JobStorage instance\n",
    "        delay: Seconds to wait between requests (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        Count of successful fetches\n",
    "    \"\"\"\n",
    "    jobs_to_fetch = storage.get_jobs_without_description()\n",
    "    total = len(jobs_to_fetch)\n",
    "    successful = 0\n",
    "    \n",
    "    print(f\"Fetching descriptions for {total} jobs...\\n\")\n",
    "    \n",
    "    for i, job in enumerate(jobs_to_fetch, 1):\n",
    "        job_title = job.get('job_title', 'Unknown')\n",
    "        job_id = job.get('job_id')\n",
    "        job_url = job.get('job_url')\n",
    "        \n",
    "        print(f\"[{i}/{total}] Fetching description for: {job_title}...\")\n",
    "        \n",
    "        try:\n",
    "            description = fetch_job_description(job_url)\n",
    "            \n",
    "            if description:\n",
    "                storage.update_job_description(job_id, description)\n",
    "                successful += 1\n",
    "                print(f\"  ✓ Success ({len(description)} chars)\")\n",
    "            else:\n",
    "                storage.update_job_description(job_id, \"ERROR\")\n",
    "                print(f\"  ✗ Failed to extract description\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            storage.update_job_description(job_id, \"ERROR\")\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "        \n",
    "        # Sleep between requests to be polite\n",
    "        if i < total:\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    print(f\"\\nCompleted: {successful}/{total} descriptions fetched successfully\")\n",
    "    return successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_pages(job_title, location, time_filter=None, limit=25):\n",
    "    \"\"\"\n",
    "    Scrape multiple pages of LinkedIn jobs.\n",
    "    \n",
    "    Args:\n",
    "        job_title: Job title to search\n",
    "        location: Location to search\n",
    "        time_filter: Not implemented yet (placeholder)\n",
    "        limit: Maximum number of jobs to fetch\n",
    "    \n",
    "    Returns:\n",
    "        List of job dictionaries (without descriptions)\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    start = 0\n",
    "    \n",
    "    while len(all_jobs) < limit:\n",
    "        url = build_linkedin_search_url(job_title, location, start)\n",
    "        \n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"Referer\": \"https://www.linkedin.com/\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                jobs = parse_job_cards(response.text)\n",
    "                \n",
    "                if not jobs:\n",
    "                    # No more jobs found\n",
    "                    break\n",
    "                \n",
    "                all_jobs.extend(jobs)\n",
    "                \n",
    "                # Stop if we've reached the limit\n",
    "                if len(all_jobs) >= limit:\n",
    "                    all_jobs = all_jobs[:limit]\n",
    "                    break\n",
    "                \n",
    "                start += 25\n",
    "                time.sleep(2)  # Be polite between page requests\n",
    "            else:\n",
    "                print(f\"Failed to fetch page at start={start}\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page: {e}\")\n",
    "            break\n",
    "    \n",
    "    return all_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_with_storage(job_title, location, time_filter=None, limit=25):\n",
    "    \"\"\"\n",
    "    Main workflow: Scrape jobs and fetch descriptions using storage.\n",
    "    \n",
    "    Args:\n",
    "        job_title: Job title to search\n",
    "        location: Location to search\n",
    "        time_filter: Time filter (not implemented)\n",
    "        limit: Maximum number of jobs to fetch\n",
    "    \n",
    "    Returns:\n",
    "        List of all jobs with descriptions\n",
    "    \"\"\"\n",
    "    # Initialize storage\n",
    "    storage = JobStorage()\n",
    "    \n",
    "    print(f\"Scraping {limit} jobs for '{job_title}' in '{location}'...\\n\")\n",
    "    \n",
    "    # Scrape job cards (without descriptions)\n",
    "    jobs = scrape_multiple_pages(job_title, location, time_filter, limit)\n",
    "    \n",
    "    # Add to storage\n",
    "    storage.add_jobs(jobs)\n",
    "    \n",
    "    # Print initial stats\n",
    "    stats = storage.get_stats()\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Found {stats['total_jobs']} jobs, fetching descriptions...\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Fetch all descriptions\n",
    "    fetch_all_descriptions(storage, delay=2)\n",
    "    \n",
    "    # Print final stats\n",
    "    final_stats = storage.get_stats()\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Final Statistics:\")\n",
    "    print(f\"  Total jobs: {final_stats['total_jobs']}\")\n",
    "    print(f\"  With descriptions: {final_stats['jobs_with_descriptions']}\")\n",
    "    print(f\"  Without descriptions: {final_stats['jobs_without_descriptions']}\")\n",
    "    print(f\"  Errors: {final_stats['jobs_with_errors']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return storage.get_all_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Scrape 10 jobs with descriptions\n",
    "print(\"Testing JobStorage with 10 Data Scientist jobs...\\n\")\n",
    "\n",
    "# Test the complete workflow\n",
    "jobs = scrape_with_storage(\"Data Scientist\", \"remote\", \"past week\", 10)\n",
    "\n",
    "# Verify results\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"✓ Total jobs returned: {len(jobs)}\")\n",
    "\n",
    "jobs_with_desc = sum(1 for j in jobs if j.get('description') and j['description'] != \"ERROR\")\n",
    "print(f\"✓ Jobs with descriptions: {jobs_with_desc}\")\n",
    "\n",
    "jobs_with_errors = sum(1 for j in jobs if j.get('description') == \"ERROR\")\n",
    "print(f\"✓ Jobs with errors: {jobs_with_errors}\")\n",
    "\n",
    "# Show sample job\n",
    "if jobs:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Sample Job (first one):\")\n",
    "    print(f\"{'='*80}\")\n",
    "    sample = jobs[0]\n",
    "    print(f\"Title: {sample['job_title']}\")\n",
    "    print(f\"Company: {sample['company']}\")\n",
    "    print(f\"Location: {sample['location']}\")\n",
    "    print(f\"Posted: {sample['posted_ago']}\")\n",
    "    print(f\"Job ID: {sample['job_id']}\")\n",
    "    \n",
    "    desc = sample.get('description', 'N/A')\n",
    "    if desc and desc != \"ERROR\":\n",
    "        desc_preview = desc[:200] + \"...\" if len(desc) > 200 else desc\n",
    "        print(f\"Description preview: {desc_preview}\")\n",
    "    else:\n",
    "        print(f\"Description: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U python-jobspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from jobspy import scrape_jobs\n",
    "\n",
    "jobs = scrape_jobs(\n",
    "    site_name=[\"indeed\", \"linkedin\", \"zip_recruiter\", \"google\"], # \"glassdoor\", \"bayt\", \"naukri\", \"bdjobs\"\n",
    "    search_term=\"software engineer\",\n",
    "    google_search_term=\"software engineer jobs near Austin, TX\",\n",
    "    location=\"Austin, TX\",\n",
    "    results_wanted=20,\n",
    "    hours_old=72,\n",
    "    country_indeed='USA',\n",
    "    \n",
    "    # linkedin_fetch_description=True # gets more info such as description, direct job url (slower)\n",
    "    # proxies=[\"208.195.175.46:65095\", \"208.195.175.45:65095\", \"localhost\"],\n",
    ")\n",
    "print(f\"Found {len(jobs)} jobs\")\n",
    "print(jobs.head())\n",
    "jobs.to_csv(\"jobs.csv\", quoting=csv.QUOTE_NONNUMERIC, escapechar=\"\\\\\", index=False) # to_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs[\"job_url\"][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "linkedin_jobs_scraper.py\n",
    "Polite Selenium scraper for public LinkedIn job search pages.\n",
    "WARNING: Do not use to bypass protections. Check robots.txt and ToS.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ---- CONFIG ----\n",
    "HEADLESS = False  # running non-headless may reduce chances of blocking during development\n",
    "MAX_PAGES = 5     # how many pages of results to fetch (be conservative)\n",
    "DELAY_MIN = 4.0\n",
    "DELAY_MAX = 8.0\n",
    "OUTPUT_FILE = \"jobs.json\"\n",
    "\n",
    "# Example LinkedIn job search URL (replace query/location as needed)\n",
    "# You can create the URL by using LinkedIn job search and copying the URL from your browser.\n",
    "# Example structure: \"https://www.linkedin.com/jobs/search/?keywords=data%20scientist&location=United%20States\"\n",
    "SEARCH_URL = \"https://www.linkedin.com/jobs/search/?keywords=software%20engineer&location=United%20States\"\n",
    "\n",
    "# ---- helpers ----\n",
    "def random_delay():\n",
    "    time.sleep(random.uniform(DELAY_MIN, DELAY_MAX))\n",
    "\n",
    "def build_driver(headless: bool = HEADLESS):\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "    # basic options\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    # Make window size deterministic\n",
    "    opts.add_argument(\"--window-size=1200,900\")\n",
    "    # Minimal privacy: do not auto-open password prompts\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=opts)\n",
    "    driver.set_page_load_timeout(60)\n",
    "    return driver\n",
    "\n",
    "def parse_job_card(card) -> Dict:\n",
    "    # This function tries multiple selectors for robustness; LinkedIn changes DOM often.\n",
    "    data = {}\n",
    "    try:\n",
    "        title_el = card.find_element(By.CSS_SELECTOR, \"a.job-card-list__title, a.job-card-list__link, h3\")\n",
    "        data[\"title\"] = title_el.text.strip()\n",
    "    except:\n",
    "        data[\"title\"] = None\n",
    "\n",
    "    try:\n",
    "        company_el = card.find_element(By.CSS_SELECTOR, \"h4.job-card-container__company-name, a.job-card-container__company-name, .job-card-container__company-name\")\n",
    "        data[\"company\"] = company_el.text.strip()\n",
    "    except:\n",
    "        # fallback\n",
    "        data[\"company\"] = None\n",
    "\n",
    "    try:\n",
    "        location_el = card.find_element(By.CSS_SELECTOR, \".job-card-container__metadata-item, .job-card-list__location\")\n",
    "        data[\"location\"] = location_el.text.strip()\n",
    "    except:\n",
    "        data[\"location\"] = None\n",
    "\n",
    "    try:\n",
    "        date_el = card.find_element(By.CSS_SELECTOR, \"time, .job-card-list__footer-wrapper span\")\n",
    "        data[\"date_posted\"] = date_el.text.strip()\n",
    "    except:\n",
    "        data[\"date_posted\"] = None\n",
    "\n",
    "    try:\n",
    "        # job link\n",
    "        a = card.find_element(By.CSS_SELECTOR, \"a.job-card-list__title, a.job-card-list__link, a\")\n",
    "        link = a.get_attribute(\"href\")\n",
    "        data[\"job_link\"] = link\n",
    "    except:\n",
    "        data[\"job_link\"] = None\n",
    "\n",
    "    try:\n",
    "        snippet = card.find_element(By.CSS_SELECTOR, \".job-card-list__snippet, .job-card-container__description-snippet\")\n",
    "        data[\"snippet\"] = snippet.text.strip()\n",
    "    except:\n",
    "        data[\"snippet\"] = None\n",
    "\n",
    "    return data\n",
    "\n",
    "def scrape_linkedin_jobs(search_url: str, max_pages: int = MAX_PAGES) -> List[Dict]:\n",
    "    driver = build_driver()\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    results = []\n",
    "\n",
    "    try:\n",
    "        driver.get(search_url)\n",
    "        random_delay()\n",
    "\n",
    "        for page_num in range(max_pages):\n",
    "            # Wait for job cards to appear\n",
    "            # Job cards are often contained in elements with 'job-card' in the class name.\n",
    "            try:\n",
    "                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"[data-job-id], .job-card-container, .job-card-list__entity\")))\n",
    "            except Exception as e:\n",
    "                print(\"No job cards found on page:\", e)\n",
    "\n",
    "            # collect cards\n",
    "            cards = driver.find_elements(By.CSS_SELECTOR, \"[data-job-id], .job-card-container, .job-card-list__entity, .result-card\")\n",
    "            print(f\"Page {page_num+1}: found {len(cards)} cards\")\n",
    "            for c in cards:\n",
    "                job = parse_job_card(c)\n",
    "                # basic dedupe by link or title+company\n",
    "                if job.get(\"job_link\") or job.get(\"title\"):\n",
    "                    results.append(job)\n",
    "\n",
    "            # Attempt to go to next page: LinkedIn has a \"next\" button or uses offset param in URL\n",
    "            # Strategy: try to click next button; if not found, try to increment start param in URL.\n",
    "            try:\n",
    "                next_btn = driver.find_element(By.CSS_SELECTOR, \"button[aria-label='Next'], button[aria-label='next'], a[aria-label='Next']\")\n",
    "                if next_btn and next_btn.is_enabled():\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_btn)\n",
    "                    random_delay()\n",
    "                    next_btn.click()\n",
    "                else:\n",
    "                    raise Exception(\"Next button not clickable\")\n",
    "            except Exception:\n",
    "                # fallback: modify URL with start param (LinkedIn uses &start=XX)\n",
    "                current_url = driver.current_url\n",
    "                # try to find start param\n",
    "                import urllib.parse as up\n",
    "                parsed = up.urlparse(current_url)\n",
    "                qs = up.parse_qs(parsed.query)\n",
    "                start = int(qs.get(\"start\", [\"0\"])[0])\n",
    "                next_start = start + 25\n",
    "                qs[\"start\"] = [str(next_start)]\n",
    "                new_q = up.urlencode(qs, doseq=True)\n",
    "                new_url = up.urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, new_q, parsed.fragment))\n",
    "                print(\"Navigating to next page URL:\", new_url)\n",
    "                driver.get(new_url)\n",
    "\n",
    "            random_delay()\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    # dedupe by job_link\n",
    "    deduped = []\n",
    "    seen = set()\n",
    "    for r in results:\n",
    "        key = r.get(\"job_link\") or (r.get(\"title\",\"\") + \"|\" + r.get(\"company\",\"\"))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(r)\n",
    "    return deduped\n",
    "\n",
    "# ---- main entrypoint ----\n",
    "if __name__ == \"__main__\":\n",
    "    Path(OUTPUT_FILE).unlink(missing_ok=True)\n",
    "    print(\"Starting scrape:\", SEARCH_URL)\n",
    "    scraped = scrape_linkedin_jobs(SEARCH_URL, max_pages=MAX_PAGES)\n",
    "    print(f\"Scraped {len(scraped)} unique job entries\")\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(scraped, f, indent=2, ensure_ascii=False)\n",
    "    print(\"Saved to\", OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def human_sleep(base_delay=3, variance=1.5):\n",
    "    \"\"\"Sleep for a random time to mimic human behavior.\"\"\"\n",
    "    delay = base_delay + random.uniform(-variance, variance)\n",
    "    time.sleep(max(delay, 1.0))  # ensure positive sleep time\n",
    "\n",
    "def scrape_linkedin_jobs(job_title, location, num_jobs=25, base_delay=4):\n",
    "    \"\"\"\n",
    "    Educational / academic version of LinkedIn job scraper.\n",
    "    Respects polite scraping practices via randomized delays.\n",
    "    \"\"\"\n",
    "    # Set up browser (headless for efficiency)\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(\"https://www.linkedin.com/jobs\")\n",
    "    human_sleep(base_delay)\n",
    "    \n",
    "    # Input fields\n",
    "    title_box = driver.find_element(By.CSS_SELECTOR, \"input[aria-label='Search by title, skill, or company']\")\n",
    "    location_box = driver.find_element(By.CSS_SELECTOR, \"input[aria-label='City, state, or zip code']\")\n",
    "    \n",
    "    # Perform search\n",
    "    title_box.clear()\n",
    "    title_box.send_keys(job_title)\n",
    "    human_sleep(base_delay)\n",
    "    location_box.clear()\n",
    "    location_box.send_keys(location)\n",
    "    location_box.send_keys(Keys.RETURN)\n",
    "    human_sleep(base_delay * 2)\n",
    "    \n",
    "    # Scroll gradually to load more jobs\n",
    "    for _ in range(3):\n",
    "        driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight / 3);\")\n",
    "        human_sleep(base_delay)\n",
    "    \n",
    "    # Capture job listings\n",
    "    job_cards = driver.find_elements(By.CSS_SELECTOR, \".jobs-search-results__list-item\")[:num_jobs]\n",
    "    human_sleep(base_delay)\n",
    "    \n",
    "    data = []\n",
    "    for idx, card in enumerate(job_cards, start=1):\n",
    "        try:\n",
    "            title = card.find_element(By.CSS_SELECTOR, \"h3\").text.strip()\n",
    "            company = card.find_element(By.CSS_SELECTOR, \".base-search-card__subtitle\").text.strip()\n",
    "            loc = card.find_element(By.CSS_SELECTOR, \".job-search-card__location\").text.strip()\n",
    "            link = card.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "            data.append({\n",
    "                \"title\": title,\n",
    "                \"company\": company,\n",
    "                \"location\": loc,\n",
    "                \"link\": link\n",
    "            })\n",
    "            print(f\"[{idx}] Scraped: {title} at {company}\")\n",
    "            human_sleep(base_delay + random.uniform(1, 3))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipped a job card due to parsing error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    driver.quit()\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting polite LinkedIn job scrape (for academic research)...\\n\")\n",
    "    df = scrape_linkedin_jobs(\"AI Engineer\", \"Boston, MA\", num_jobs=10)\n",
    "    df.to_csv(\"linkedin_jobs_sample.csv\", index=False)\n",
    "    print(\"\\nDone. Results saved to linkedin_jobs_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"LINKEDIN_EMAIL\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "outfit-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
